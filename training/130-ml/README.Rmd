---
title: "Machine Learning"
author: "Daniel Chen"
date: ""
output: # html_document
  md_document:
    variant: markdown_github
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> The fact that data science exists as a field is a colossal failure of statistics. To me, that is what statistics is all about. It is gaining insight from data using modelling and visualization. Data munging and manipulation is hard and statistics has just said that’s not our domain.”

- Hadley Wickham, PhD

https://priceonomics.com/hadley-wickham-the-man-who-revolutionized-r/

```{r}
diamonds <- ggplot2::diamonds
economics <- ggplot2::economics
```

# Introduction

Understanding your dataset is more important than fitting a model and calling it a day.

https://rpubs.com/neilfws/91339

```{r}
data(anscombe)
summary(anscombe)
```

```{r}
sapply(1:4, function(x) cor(anscombe[, x], anscombe[, x+4]))
```

```{r}
sapply(5:8, function(x) var(anscombe[, x]))
```

```{r}
lm(y1 ~ x1, data = anscombe)
```

```{r}
library(ggplot2)
library(gridExtra)

p1 <- ggplot(anscombe) +
  geom_point(aes(x1, y1), color = "darkorange", size = 3) +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") +
  expand_limits(x = 0, y = 0) +
  labs(title = "dataset 1")

p2 <- ggplot(anscombe) +
  geom_point(aes(x2, y2), color = "darkorange", size = 3) +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") +
  expand_limits(x = 0, y = 0) +
  labs(title = "dataset 2")

p3 <- ggplot(anscombe) +
  geom_point(aes(x3, y3), color = "darkorange", size = 3) +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") +
  expand_limits(x = 0, y = 0) +
  labs(title = "dataset 3")

p4 <- ggplot(anscombe) +
  geom_point(aes(x4, y4), color = "darkorange", size = 3) +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") +
  expand_limits(x = 0, y = 0) +
  labs(title = "dataset 4")

sdalr::multiplot(p1, p2, p3, p4, cols = 2)
```

## anscombosaurus

```{r}
data("anscombosaurus", package = 'sdalr')

summary(anscombosaurus)
```

# Summary Statistics

```{r}
summary(diamonds$price)
```

```{r}
summary(diamonds)
```

```{r}
mean(diamonds$price)
sd(diamonds$price)
quantile(diamonds$price, probs = 0.25)
```

## Correlation

$$
r_{xy} = \dfrac{\sum_{i = 1}^n (x_i - \bar{x}) (y_i - \bar{y})}{(n - 1) s_x s_y}
$$

```{r}
cor(economics$pce, economics$psavert)
```

```{r}
GGally::ggpairs(data = dplyr::select(economics, -date, -pop))
```

```{r}
GGally::wrap(
  GGally::ggpairs(data = dplyr::select(economics, -date, -pop)),
  labelSize = 8
)
```

## Covariance

$$
cov(X, Y) = \dfrac{1}{N - 1} \sum_{i = 1}^N (x_i - \bar{x})(y_i - \bar{y})
$$

```{r}
cov(economics$pce, economics$psavert)
```

```{r}
identical(
  cov(economics$pce, economics$psavert),
  cor(economics$pce, economics$psavert) *
    sd(economics$pce) *
    sd(economics$psavert)
)
```

# Linear Models

```{r}
data(diamonds, package = 'ggplot2')
```

```{r}
head(diamonds)
```


```{r}
glm(formula = price ~ carat, data = diamonds)
```

```{r}
glm(formula = price ~ carat + depth + table + x + y + z, data = diamonds)
```

```{r}
m <- glm(formula = price ~ carat + depth + table + x + y + z, data = diamonds)
summary(m)
```

```{r}
library(broom)

broom::tidy(m)

head(broom::augment(m))

broom::glance(m)
```

```{r}
plot(m)
```

# Generalized Linear Models

## Logistic

```
[, 1]	 mpg	 Miles/(US) gallon
[, 2]	 cyl	 Number of cylinders
[, 3]	 disp	 Displacement (cu.in.)
[, 4]	 hp	 Gross horsepower
[, 5]	 drat	 Rear axle ratio
[, 6]	 wt	 Weight (1000 lbs)
[, 7]	 qsec	 1/4 mile time
[, 8]	 vs	 V/S
[, 9]	 am	 Transmission (0 = automatic, 1 = manual)
[,10]	 gear	 Number of forward gears
[,11]	 carb	 Number of carburetors
```

```{r}
mpg_bin <- function(mpg) {
  if (mpg > 22) {
    return('good')
  } else {
    return('poor')
  }
}

mpg_bin_int <- function(mpg) {
  if (mpg > 22) {
    return(1)
  } else {
    return(0)
  }
}

mtcars$mpg_bin <- sapply(X = mtcars$mpg, FUN = mpg_bin)
mtcars$mpg_bin_int <- sapply(X = mtcars$mpg, FUN = mpg_bin_int)
head(mtcars)
```

```{r}
m <- glm(formula = as.factor(mpg_bin) ~ cyl + hp + wt + as.factor(am),
         data = mtcars,
         family = binomial(link = 'logit'))
summary(m)
broom::tidy(m)
```

```{r}
m <- glm(formula = as.factor(mpg_bin_int) ~ cyl + hp + wt + as.factor(am),
         data = mtcars,
         family = binomial(link = 'logit'))
summary(m)
broom::tidy(m)
```

```{r}
results  <- broom::tidy(m)
results$or <- exp(results$estimate)
results
```

## Survival Analysis

https://rpubs.com/daspringate/survival

```{r}
library(survival)
head(heart)
```

Survival of patients on the waiting list for the Stanford heart transplant program.

```
start, stop, event:	 Entry and exit time and status for this interval of time
age:	 age-48 years
year:	 year of acceptance (in years after 1 Nov 1967)
surgery:	 prior bypass surgery 1=yes
transplant:	 received transplant 1=yes
id:	 patient id
```

```{r}
s <- Surv(heart$start, heart$stop, heart$event)
```

```{r}
cox <- coxph(Surv(heart$start, heart$stop, heart$event) ~ age + year + as.factor(surgery) + as.factor(transplant),
             data = heart)
summary(cox)
```

```{r}
plot(survfit(cox), xlab = 'days', ylab = 'Survival Rate', conf.int = TRUE)
```

```{r}
cox <- coxph(Surv(heart$start, heart$stop, heart$event) ~ age + year + strata(as.factor(surgery)) + as.factor(transplant),
             data = heart)
summary(cox)
plot(survfit(cox), xlab = 'days', ylab = 'Survival Rate', conf.int = TRUE, col = 1:2)
legend("bottomleft", legend = c(1, 2), lty = 1, col = 1:2, text.col = 1:2, title = 'rx')
```

# Model Diagnostics

```{r}
m1 <- glm(price ~ carat, data = diamonds)
m2 <- glm(price ~ carat + as.factor(cut), data = diamonds)
m3 <- glm(price ~ carat + as.factor(cut) + depth, data = diamonds)
m4 <- glm(price ~ carat + as.factor(cut) + depth +  table, data = diamonds)
```

```{r}
head(fortify(m1))
```

```{r}
plot(m1)
```

```{r}
anova(m1, m2, m3, m4)
```

```{r}
AIC(m1, m2, m3, m4)
```

```{r}
BIC(m1, m2, m3, m4)
```

## Cross Validation

```{r}
library(boot)

cv1 <- cv.glm(data = diamonds, glmfit = m1, K = 5)

# raw cross-validation error based on MSE
# adjusted cross-validation error
cv1$delta
```

```{r}
cv2 <- cv.glm(data = diamonds, glmfit = m2, K = 5)
cv3 <- cv.glm(data = diamonds, glmfit = m3, K = 5)
cv4 <- cv.glm(data = diamonds, glmfit = m4, K = 5)
```

```{r}
cv_results <- as.data.frame(rbind(cv1$delta, cv2$delta, cv3$delta, cv4$delta))
names(cv_results) <- c('error', 'adjusted_error')
cv_results$model <- sprintf('model_%s', 1:4)
cv_results
```

# Bootstrap

# Regularization

## LASSO

```{r}
head(mtcars)
```

```{r}
library(useful)
mtcars_x <- build.x(mpg_bin_int ~ mpg + cyl + disp + hp + drat + wt + qsec +
                      as.factor(vs) + as.factor(am) + as.factor(gear) + as.factor(carb),
                    data = mtcars, contrasts = FALSE)
mtcars_y <- build.y(mpg_bin_int ~ mpg + cyl + disp + hp + drat + wt + qsec +
                      as.factor(vs) + as.factor(am) + as.factor(gear) + as.factor(carb),
                    data = mtcars)
```

```{r}
library(glmnet)
set.seed(42)
```

```{r}
cars_lasso <- cv.glmnet(x = mtcars_x, y = mtcars_y, family = "binomial", nfolds = 5)
plot(cars_lasso)
```


```{r}
cars_lasso$lambda.min
cars_lasso$lambda.1se
```


```{r}
coef(cars_lasso, s = "lambda.1se")
```

```{r}
plot(cars_lasso$glmnet.fit, xvar = "lambda")
```


## Ridge

```{r}
set.seed(42)
cars_ridge <- cv.glmnet(x = mtcars_x, y = mtcars_y, family = "binomial", nfolds = 5, alpha = 0)
```

```{r}
plot(cars_ridge)
cars_ridge$lambda.min
cars_ridge$lambda.1se
coef(cars_ridge, s = "lambda.1se")
plot(cars_ridge$glmnet.fit, xvar = "lambda")
```

## Parallelization and optimizing alpha

```{r}
library(parallel)
library(doParallel)
set.seed(42)

# building a 2 layered CV
# buld a vector specifying membership for each fold
# to make sure an observation is on the same fold
folds <- sample(rep(x = 1:5, length.out = nrow(mtcars_x)))

# alphas
alphas <- seq(from = 0.5, to = 1, by = 0.05)
```

```{r}
set.seed(42)
cl <- makeCluster(2)
registerDoParallel(cl)

par_results <- foreach(i = 1:length(alphas),
                       .errorhandling = 'remove',
                       .inorder = FALSE,
                       .multicombine = TRUE,
                       .export = c("mtcars_x", "mtcars_y", 'alphas', 'folds'),
                       .packages = 'glmnet') %dopar% {
                         print(alphas[i])
                         cv.glmnet(x = mtcars_x, y = mtcars_y, family = 'binomial',
                                   nfolds = 5, foldid = folds, alpha = alphas[i])
                       }
stopCluster(cl)
```

```{r}
sapply(par_results, class)
```

```{r}
extract_glmnet <- function(glmnet_object) {
  lambda_min <- glmnet_object$lambda.min
  lambda_1se <- glmnet_object$lambda.1se
  
  min_i <- which(glmnet_object$lambda == lambda_min)
  se1_i <- which(glmnet_object$lambda == lambda_1se)
  
  data.frame(lambda_min = lambda_min, error_min = glmnet_object$cvm[min_i],
             lambda_1se = lambda_1se, error_1se = glmnet_object$cvm[se1_i])
}

ldf <- lapply(par_results, extract_glmnet)

# using base R Reduce function
en_alpha <- Reduce(rbind, ldf)
en_alpha

# tidyverse
en_alpha <- purrr::reduce(ldf, rbind)
en_alpha
```

```{r}
en_alpha$alpha <- alphas

en_alpha
```

```{r}
library(ggplot2)
library(tidyr)

gathered <- gather(en_alpha, type, lambda, -alpha, -error_min, -error_1se)
gathered <- gather(gathered, error_type, error, -alpha, -type, -lambda)

ggplot(data = gathered, aes(x = alpha, y = error)) + geom_point() + facet_grid(type ~ error_type)
```

# Decision Trees

```{r}
library(rpart)

mtcars_tree <- rpart(mpg ~ cyl + disp + hp + drat + wt + qsec +
                        as.factor(vs) +as.factor(am) + as.factor(gear) + as.factor(carb),
                      data = mtcars)
mtcars_tree
```

```{r}
library(rpart.plot)
rpart.plot(mtcars_tree)
```

```{r}
mtcars_tree_bin <- rpart(as.factor(mpg_bin) ~ cyl + disp + hp + drat + wt + qsec +
                        as.factor(vs) +as.factor(am) + as.factor(gear) + as.factor(carb),
                      data = mtcars)
mtcars_tree_bin
rpart.plot(mtcars_tree_bin)
```

# Random Forest

Ensemble method

```{r}
library(randomForest)
library(useful)

rf_formula <- as.factor(mpg_bin) ~ cyl + disp + hp + drat + wt + qsec +
  as.factor(vs) +as.factor(am) + as.factor(gear) + as.factor(carb)

rf_x <- build.x(rf_formula, data = mtcars)
rf_y <- build.y(rf_formula, data = mtcars)

mtcars_rf <- randomForest(x = rf_x, y = rf_y)
mtcars_rf
```

# Clustering

## Kmeans (Unsupervised Learning)

https://stats.stackexchange.com/questions/56500/what-are-the-main-differences-between-k-means-and-k-nearest-neighbours

K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.

K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points.

```{r}
head(iris)
iris_subset <- iris[, !names(iris) %in% c('Species')]
head(iris_subset)
```

```{r}
set.seed(42)

iris_k3 <- kmeans(x = iris_subset, centers = 3)
iris_k3
```

```{r}
plot.kmeans(iris_k3, data = iris_subset)
```

```{r}
plot.kmeans(iris_k3, data = iris, class = 'Species')
```

```{r}
best_iris <- useful::FitKMeans(iris_subset, max.clusters = 25, nstart = 10, seed = 42)
best_iris
```

```{r}
PlotHartigan(best_iris)
```

